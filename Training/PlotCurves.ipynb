{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "A lot of functions are defined. They can be split up in several parts:\n",
    "1. Loading of the data. Loads the right data and the parameters etc\n",
    "2. Training of the model. Trains the selected model.\n",
    "3. Calculation of the CI and the pvalue of AUC: As it states\n",
    "4. Plotting algorithms: Plot the ROC curves, PR curves, Feature importance bars and number needed to screen\n",
    "\n",
    "These are called in the final section and gives the results.\n",
    "\n",
    "The algorithms are tested on a server online, indicating the need for installing several modules, as well as refering to ./datafolder, where the files were stored. For replication, this has to be altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install needed modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install import-ipynb\n",
    "%pip install sklearn\n",
    "%pip install xgboost\n",
    "%pip install mlxtend\n",
    "#!pip install pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the data, FFS and hyperparameter info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def loadmodules(setNumber, modelName):\n",
    "    \n",
    "    # First, a test split is made (the same as during training). On this test data, no training has been performed.\n",
    "    # After, extra data is loaded, from which extra controls are added to the test data to increase the ratio cases:controls \n",
    "    \n",
    "    with open('./datafolder/DataMatrix{}.pickle'.format(setNumber), 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "        X.replace(True,1, inplace = True)\n",
    "        X.replace(False,0, inplace = True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify = y)\n",
    "    del X, y\n",
    "\n",
    "    with open('./datafolder/DataMatrix{}_extra.pickle'.format(setNumber), 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "        X.replace(True,1, inplace = True)\n",
    "        X.replace(False,0, inplace = True)\n",
    "    \n",
    "    from random import sample, seed\n",
    "    \n",
    "    # Mogelijk is er een andere ratio (bijv 1:5) gebruikt bij training\n",
    "    # Hier worden die controles verwijderd uit de mogelijke lijst met controles (voor de 20% test set)\n",
    "    # Als useNewRatio = True, trainen we hier ook met een 1:5 ratio. Doen we hier niet\n",
    "    useNewRatio = False\n",
    "    new_ratio=5\n",
    "    len_cases = y_train['target'].sum()\n",
    "    seed(0)\n",
    "    toremove_list = sample(list(X.index),len_cases*(new_ratio-1))\n",
    "    \n",
    "    if useNewRatio==True:\n",
    "        X_train = pd.concat([X_train, X.loc[toremove_list]])\n",
    "        y_train = pd.concat([y_train, y.loc[toremove_list]])\n",
    "        \n",
    "    X.drop(index = toremove_list, inplace = True)\n",
    "    y.drop(index = toremove_list, inplace = True)\n",
    "    \n",
    "    # Selecteer at random precies 44 controles (= 1:45 ratio) voor de test set\n",
    "    len_cases = y_test['target'].sum()\n",
    "    seed(0)\n",
    "    tosample_list = sample(list(X.index),len_cases*44)\n",
    "    X = X.loc[tosample_list]\n",
    "    y = y.loc[tosample_list]\n",
    "    \n",
    "    # Alle extras zijn de 44: Voor trainen wordt dit niet gebruikt, bij testen worden deze bijgevoegd\n",
    "    X_train_extra = X.copy()\n",
    "    X_test_extra = X.copy()\n",
    "    y_train_extra = y.copy()\n",
    "    y_test_extra = y.copy()\n",
    "    \n",
    "    del X, y\n",
    "    \n",
    "    with open('./datafolder/FFS_set{}_full_{}'.format(setNumber,modelName),'rb') as handle:\n",
    "        pipe = pickle.load(handle)\n",
    "    \n",
    "    # Find the best parameters as selected by FFS\n",
    "    full_dict = pipe['sfs'].get_metric_dict()\n",
    "    max_ind = max(full_dict.keys(), key=(lambda key: full_dict[key]['avg_score']))\n",
    "    best_features = full_dict[max_ind]['feature_names']\n",
    "    best_features_names = best_features\n",
    "    best_features = [int(x) for x in best_features]\n",
    "    \n",
    "    # Make datasets with only the best parameters\n",
    "    X_train = X_train.iloc[:,best_features]\n",
    "    y_train = y_train.iloc[:,0]\n",
    "    X_test = X_test.iloc[:,best_features]\n",
    "    y_test = y_test.iloc[:,0]\n",
    "    \n",
    "    X_train_extra = X_train_extra.iloc[:,best_features]\n",
    "    y_train_extra = y_train_extra.iloc[:,0]\n",
    "    X_test_extra = X_test_extra.iloc[:,best_features]\n",
    "    y_test_extra = y_test_extra.iloc[:,0]\n",
    "    \n",
    "    with open('./datafolder/Tuning_set{}_{}'.format(setNumber, modelName),'rb') as handle:\n",
    "        gridsearch = pickle.load(handle)\n",
    "        \n",
    "    best_params = {x.replace(\"classifier__\", \"\"): v for x, v in gridsearch.best_params_.items()}\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test, best_params, pipe, X_train_extra, X_test_extra, y_train_extra, y_test_extra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the CI and pvalue of the AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAUC_CI(AUC, N1, N2):\n",
    "    # Calculates the confidence interval of a ROC curve\n",
    "    import math\n",
    "    Q1 = AUC / (2-AUC)\n",
    "    Q2 = (2*math.pow(AUC,2))/(1+AUC)\n",
    "    \n",
    "    part1 = AUC * (1-AUC)\n",
    "    part2 = (N1-1) * (Q1-math.pow(AUC,2))\n",
    "    part3 = (N2-1) * (Q2-math.pow(AUC,2))\n",
    "    part4 = N1*N2\n",
    "    \n",
    "    SE_AUC = math.sqrt((part1+part2+part3)/part4) \n",
    "    \n",
    "    ci = [AUC - 1.96*SE_AUC, AUC + 1.96*SE_AUC]\n",
    "    \n",
    "    return ci\n",
    "\n",
    "def calc_pvalue_AUCs(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    The implementation of DeLong's method is used as from:\n",
    "    https://github.com/yandexdataschool/roc_comparison/tree/44fcd23c998b39f30440d833a0820f1c7e6d8bc7\n",
    "    as on 18 nov 2021.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import scipy.stats\n",
    "\n",
    "    def compute_midrank(x):\n",
    "        \"\"\"Computes midranks.\n",
    "        Args:\n",
    "           x - a 1D numpy array\n",
    "        Returns:\n",
    "           array of midranks\n",
    "        \"\"\"\n",
    "        J = np.argsort(x)\n",
    "        Z = x[J]\n",
    "        N = len(x)\n",
    "        T = np.zeros(N, dtype=np.float)\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            j = i\n",
    "            while j < N and Z[j] == Z[i]:\n",
    "                j += 1\n",
    "            T[i:j] = 0.5*(i + j - 1)\n",
    "            i = j\n",
    "        T2 = np.empty(N, dtype=np.float)\n",
    "        # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "        # instead of 1-based in the AUC formula in the paper\n",
    "        T2[J] = T + 1\n",
    "        return T2\n",
    "\n",
    "\n",
    "    def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "        \"\"\"\n",
    "        The fast version of DeLong's method for computing the covariance of\n",
    "        unadjusted AUC.\n",
    "        Args:\n",
    "           predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "              sorted such as the examples with label \"1\" are first\n",
    "        Returns:\n",
    "           (AUC value, DeLong covariance)\n",
    "        Reference:\n",
    "         @article{sun2014fast,\n",
    "           title={Fast Implementation of DeLong's Algorithm for\n",
    "                  Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n",
    "           author={Xu Sun and Weichao Xu},\n",
    "           journal={IEEE Signal Processing Letters},\n",
    "           volume={21},\n",
    "           number={11},\n",
    "           pages={1389--1393},\n",
    "           year={2014},\n",
    "           publisher={IEEE}\n",
    "         }\n",
    "        \"\"\"\n",
    "        # Short variables are named as they are in the paper\n",
    "        m = label_1_count\n",
    "        n = predictions_sorted_transposed.shape[1] - m\n",
    "        positive_examples = predictions_sorted_transposed[:, :m]\n",
    "        negative_examples = predictions_sorted_transposed[:, m:]\n",
    "        k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "        tx = np.empty([k, m], dtype=np.float)\n",
    "        ty = np.empty([k, n], dtype=np.float)\n",
    "        tz = np.empty([k, m + n], dtype=np.float)\n",
    "        for r in range(k):\n",
    "            tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "            ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "            tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "        aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "        v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "        v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "        sx = np.cov(v01)\n",
    "        sy = np.cov(v10)\n",
    "        delongcov = sx / m + sy / n\n",
    "        return aucs, delongcov\n",
    "\n",
    "\n",
    "    def calc_pvalue(aucs, sigma):\n",
    "        \"\"\"Computes log(10) of p-values.\n",
    "        Args:\n",
    "           aucs: 1D array of AUCs\n",
    "           sigma: AUC DeLong covariances\n",
    "        Returns:\n",
    "           log10(pvalue)\n",
    "        \"\"\"\n",
    "        l = np.array([[1, -1]])\n",
    "        z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "        return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "    def compute_ground_truth_statistics(ground_truth):\n",
    "        assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "        order = (-ground_truth).argsort()\n",
    "        label_1_count = int(ground_truth.sum())\n",
    "        return order, label_1_count\n",
    "\n",
    "\n",
    "    def delong_roc_variance(ground_truth, predictions):\n",
    "        \"\"\"\n",
    "        Computes ROC AUC variance for a single set of predictions\n",
    "        Args:\n",
    "           ground_truth: np.array of 0 and 1\n",
    "           predictions: np.array of floats of the probability of being class 1\n",
    "        \"\"\"\n",
    "        order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "        predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "        aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "        assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "        return aucs[0], delongcov\n",
    "\n",
    "\n",
    "    def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "        \"\"\"\n",
    "        Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "        Args:\n",
    "           ground_truth: np.array of 0 and 1\n",
    "           predictions_one: predictions of the first model,\n",
    "              np.array of floats of the probability of being class 1\n",
    "           predictions_two: predictions of the second model,\n",
    "              np.array of floats of the probability of being class 1\n",
    "        \"\"\"\n",
    "        order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "        predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "        aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "        return calc_pvalue(aucs, delongcov)\n",
    "    \n",
    "    # Actual call to definitions\n",
    "    pvalue = 10**delong_roc_test(ground_truth, predictions_one, predictions_two)\n",
    "    return pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def trainModel(X_train, y_train, X_test, y_test, X_train_extra, X_test_extra, y_train_extra, y_test_extra, best_params, modelName, ratio = '1to1', teston = '1to1', withcv = False, sortCurve = 'AUC'):\n",
    "\n",
    "    # Make the model\n",
    "    categorical = list(X_train.select_dtypes('category').columns)\n",
    "    numerical = list(X_train.select_dtypes('number').columns)\n",
    "    transformer = ColumnTransformer(transformers=[('cat', StandardScaler(), categorical),\n",
    "                                                  ('num', StandardScaler(), numerical)])\n",
    "\n",
    "    if modelName == 'LR':\n",
    "        classifier = LogisticRegression(**best_params, random_state = 42)\n",
    "    elif modelName == 'RF':\n",
    "        classifier = RandomForestClassifier(**best_params, random_state = 42)\n",
    "    elif modelName == 'XGB':\n",
    "        classifier = xgb.XGBClassifier(**best_params, random_state = 42)\n",
    "    \n",
    "    # A model is always trained first on non-upsampled training data\n",
    "    clf = Pipeline([('preprocessing', transformer), ('classifier',classifier)])\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    # Afterwards, data is upsampled, to predict on (upsampled) training data (we dont use this) and upsampled test data (all data from this is unseen)\n",
    "    if ratio == 'upsampled':\n",
    "        X_train = pd.concat([X_train, X_train_extra])\n",
    "        X_test = pd.concat([X_test, X_test_extra])\n",
    "        y_train = pd.concat([y_train, y_train_extra])\n",
    "        y_test = pd.concat([y_test, y_test_extra])\n",
    "    \n",
    "    # Predict the outcome\n",
    "    y_pred_train = clf.predict_proba(X_train)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    \n",
    "    # Create an ROC or an precision recall curve\n",
    "    if sortCurve == 'AUC':\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_pred_train[:, 1])\n",
    "        fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_test[:, 1])\n",
    "        fpr_train[-1] = 1.0\n",
    "        fpr_test[-1] = 1.0\n",
    "        \n",
    "    elif sortCurve == 'PR':\n",
    "        tpr_train, fpr_train, thresholds_train = precision_recall_curve(y_train, y_pred_train[:, 1])\n",
    "        tpr_test, fpr_test, thresholds_test = precision_recall_curve(y_test, y_pred_test[:, 1])\n",
    "        fpr_train[0] = 1.0\n",
    "        fpr_test[0] = 1.0\n",
    "    \n",
    "    tpr_train[-1]=1.0\n",
    "    tpr_test[-1]=1.0\n",
    "    \n",
    "    # Get the AUC score\n",
    "    auc_train = auc(fpr_train, tpr_train)\n",
    "    auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    return [fpr_train, tpr_train, thresholds_train, auc_train, y_pred_train[:,1], y_train], [fpr_test, tpr_test, thresholds_test, auc_test, y_pred_test[:,1], y_test], clf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the AUC or PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "#matplotlib notebook\n",
    "\n",
    "def plotCurve(plotInfo, curve = 'test', sortCurve = 'AUC'):\n",
    "    \n",
    "    # Plot the ROC or PR curve\n",
    "    # All data is stored in the plotInfo, so it is just plotting\n",
    "\n",
    "    plots = []\n",
    "    auc = {}\n",
    "    curvedict = {'test':1, 'train':0}\n",
    "\n",
    "    for i in plotInfo.keys():\n",
    "\n",
    "        plots.append(plt.plot(plotInfo[i][curvedict[curve]][0], plotInfo[i][curvedict[curve]][1], \n",
    "                lw = 2, label = 'Data({})set {}, AUC = {}'.format(curve, i, str(plotInfo[i][curvedict[curve]][3])[0:6])))\n",
    "        \n",
    "        ind = np.argmax(plotInfo[i][curvedict[curve]][1]-plotInfo[i][curvedict[curve]][0])\n",
    "        \n",
    "        if sortCurve =='AUC':\n",
    "            thres = plotInfo[i][curvedict[curve]][2][ind] \n",
    "            y_pred_bin = [1 if x>=thres else 0 for x in plotInfo[i][curvedict[curve]][4]]\n",
    "            cf_mat = confusion_matrix(plotInfo[i][curvedict[curve]][5], y_pred_bin)\n",
    "            print('{}'.format(i))\n",
    "            print(cf_mat)\n",
    "            print('Accuracy: {}'.format((cf_mat[0,0]+cf_mat[1,1])/np.sum(cf_mat)))\n",
    "            print('Sensitivity: {}'.format(plotInfo[i][curvedict[curve]][1][ind]))\n",
    "            print('Specificity: {}'.format(1-plotInfo[i][curvedict[curve]][0][ind]))\n",
    "            \n",
    "            n1 = plotInfo[i][curvedict[curve]][5].sum()\n",
    "            n2 = len(plotInfo[i][curvedict[curve]][5]) - n1\n",
    "            CI = calcAUC_CI(plotInfo[i][curvedict[curve]][3], n1, n2)\n",
    "            print('Confidence interval: {}', CI)\n",
    "    \n",
    "    if sortCurve =='AUC':\n",
    "        plt.xlabel('1-Specificity')\n",
    "        plt.ylabel('Sensitivity')\n",
    "        plots.append(plt.plot([0,1],[0,1], lw = 2, label = 'Random'))\n",
    "        \n",
    "    elif sortCurve == 'PR':\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plots.append(plt.plot([0,1],[plotInfo[i][1][5].sum()/plotInfo[i][1][5].size,plotInfo[i][1][5].sum()/plotInfo[i][1][5].size], lw = 2, label = 'Random'))\n",
    "    \n",
    "    legax = []\n",
    "    for numax in plots:\n",
    "        legax.append(numax[0])\n",
    "\n",
    "    plt.legend(handles = legax)\n",
    "    \n",
    "    plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
    "    plt.title('1:45 ratio, -24 to -12 months')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of patients called up vs number of patients identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "def plotNumtoscreenCurve(plotInfo, curve = 'test'):\n",
    "    \n",
    "    # Plot the number needed to screen based on the ROC curve.\n",
    "    # From the ROC curve, sensitivity and specificity at each point is given.\n",
    "    # Based on sens and spec, the total num of patients correctly screened vs. faulty positives can be derived\n",
    "\n",
    "    plots = []\n",
    "    curvedict = {'test':1, 'train':0}\n",
    "\n",
    "    for i in plotInfo.keys():\n",
    "\n",
    "        toScreen = [[],[]]\n",
    "        for j in plotInfo[i][curvedict[curve]][2]:\n",
    "            y_pred_bin = [1 if x>=j else 0 for x in plotInfo[i][curvedict[curve]][4]]\n",
    "            cf_mat = confusion_matrix(plotInfo[i][curvedict[curve]][5], y_pred_bin)\n",
    "            toScreen[0].append(cf_mat[1][1])\n",
    "            toScreen[1].append(cf_mat[0][1] + cf_mat[1][1])\n",
    "            \n",
    "        plots.append(plt.plot(toScreen[1], toScreen[0], \n",
    "                lw = 2, label = 'Data({})set {}'.format(curve, i)))\n",
    "        \n",
    "        aantal_pat = toScreen[0][-1]\n",
    "        \n",
    "        ten_p = math.floor(aantal_pat/10)\n",
    "        for ind,values in enumerate(toScreen[0]):\n",
    "            if values >= ten_p:\n",
    "                ind_ten = ind\n",
    "                break\n",
    "        print('Total pat: {}. At 10 percent: {} pat diagnosed, {} pat wrong'.format(aantal_pat, toScreen[1][ind_ten], toScreen[1][ind_ten]- ten_p))\n",
    "        \n",
    "    legax = []\n",
    "    for numax in plots:\n",
    "        legax.append(numax[0])\n",
    "\n",
    "    plt.legend(handles = legax)\n",
    "    plt.xlabel('Patients classified positive (n)')\n",
    "    plt.ylabel('Early diagnosed patients (n)')\n",
    "    plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
    "    plt.title('1:45 ratio, -24 to -12 months')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureImportance(pipe, clf):\n",
    "    \n",
    "    # Plot the most important features as given by the model.\n",
    "    # This part is not yet completly finished\n",
    "    # Works for the LR (which is needed), still needs work for XGB\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    full_dict = pipe['sfs'].get_metric_dict()\n",
    "\n",
    "    paramInOrder = []\n",
    "    for i in full_dict.keys():\n",
    "        for j in full_dict[i]['feature_idx']:\n",
    "            if j not in paramInOrder:\n",
    "                paramInOrder.append(j)\n",
    "\n",
    "    allparam = pipe['preprocessing'].transformers[1][2]      \n",
    "    FFSorder = [allparam[x] for x in paramInOrder]\n",
    "    included_par = FFSorder[0:len(allparam)]\n",
    "\n",
    "    importance = clf['classifier'].coef_\n",
    "    #importance = clf['classifier'].feature_importances_\n",
    "    sortind = np.argsort(abs(importance[0]))[-20::]\n",
    "    #sortind = np.argsort(abs(importance))[-20::]\n",
    "    yax = importance[0,sortind]\n",
    "    #yax = importance[sortind]\n",
    "    xax = [included_par[x] for x in sortind]\n",
    "    plt.barh(xax,yax)\n",
    "    plt.xlabel('Feature importance (weight)')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    #sortind = np.argsort(abs(importance[0]))[-20::]\n",
    "    #yax = importance[0,sortind]\n",
    "    #xax = [included_par[x] for x in sortind]\n",
    "    #plt.barh(xax,yax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the section which you run to start the plotting\n",
    "plotInfo={}\n",
    "\n",
    "# Ratio can be 1to1 or upsampled\n",
    "# SortCurve can be AUC or PR\n",
    "# Curve can be on test set or on training set\n",
    "ratio = 'upsampled'\n",
    "sortCurve = 'AUC'\n",
    "curve = 'test'\n",
    "\n",
    "for setNum in [1,2,3]:\n",
    "    for modelName in ['LR','RF','XGB']:\n",
    "        X_train, y_train, X_test, y_test, best_params, pipe, X_train_extra, X_test_extra, y_train_extra, y_test_extra = loadmodules(setNum, modelName)\n",
    "        Model_train,Model_test, clf = trainModel(X_train, y_train, X_test, y_test, X_train_extra, X_test_extra, y_train_extra, y_test_extra, best_params, modelName, ratio=ratio, sortCurve = sortCurve)\n",
    "        plotInfo[str(setNum) + modelName] = [Model_train, Model_test]\n",
    "        \n",
    "        # Manually alter predictions (which is needed for the pvalue of the AUC) vs the current one. \n",
    "        # i.e. Run the best algorithm, save as predictions, and then comment that section and run a different model and print the pvalue\n",
    "        \n",
    "        #predictions = Model_test[4] # best\n",
    "        #print(calc_pvalue_AUCs(Model_test[5], predictions, Model_test[4])) # Current\n",
    "\n",
    "# Select what you want to do: Plot the ROC, plot the patients screened vs identified, plot feature importance, and save if needed\n",
    "\n",
    "plotCurve(plotInfo, curve = curve, sortCurve = sortCurve)\n",
    "#plotNumtoscreenCurve(plotInfo, curve = curve)\n",
    "#plotFeatureImportance(pipe, clf)\n",
    "#plt.savefig('XGB_1to30_2.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section can be used for plotting histograms of false / true negatives / positives\n",
    "'''Under construction\n",
    "totalPat = 1709\n",
    "plotname = '3LR'\n",
    "\n",
    "y_pred = plotInfo[plotname][1][4]\n",
    "y_true = plotInfo[plotname][1][5]\n",
    "X_test = pd.concat([X_test, X_test_extra])\n",
    "\n",
    "import math\n",
    "patToPredict = math.floor(totalPat/10)\n",
    "\n",
    "for ind,i in enumerate(plotInfo[plotname][1][1]):\n",
    "    if i >= (patToPredict/(patToPredict+(totalPat-patToPredict))):\n",
    "        thresind = ind\n",
    "        break\n",
    "\n",
    "thres = plotInfo[plotname][1][2][thresind]\n",
    "truepos = []\n",
    "falsepos = []\n",
    "trueneg = []\n",
    "falseneg = []\n",
    "overig=[]\n",
    "\n",
    "for ind, i in enumerate(y_pred):\n",
    "    if i>=thres and y_true.iloc[ind]==1:\n",
    "        truepos.append(ind)\n",
    "    elif i>=thres and y_true.iloc[ind]==0:\n",
    "        falsepos.append(ind)\n",
    "    elif i<thres and y_true.iloc[ind]==0:\n",
    "        trueneg.append(ind)\n",
    "    elif i<thres and y_true.iloc[ind]==1:s\n",
    "        falseneg.append(ind)\n",
    "    else:\n",
    "        overig.append(ind)\n",
    "        \n",
    "fig, ax = plt.subplots(2,2, sharex = 'col')\n",
    "ax[0,0].hist(X_test.iloc[truepos,0], bins = range(60,115,2))\n",
    "ax[0,1].hist(X_test.iloc[falsepos,0], bins = range(60,115,2))\n",
    "ax[1,0].hist(X_test.iloc[trueneg,0], bins = range(60,115,2))\n",
    "ax[1,1].hist(X_test.iloc[falseneg,0], bins = range(60,115,2))\n",
    "\n",
    "ax[0,0].set_ylabel('Number of patients (n)')\n",
    "ax[1,0].set_ylabel('Number of patients (n)')\n",
    "\n",
    "ax[1,0].set_xlabel('Age in bins of 2 years')\n",
    "ax[1,1].set_xlabel('Age in bins of 2 years')\n",
    "\n",
    "ax[0,0].set_title('True positive patients')\n",
    "ax[0,1].set_title('False positive patients')\n",
    "ax[1,0].set_title('True negative patients')\n",
    "ax[1,1].set_title('False negative patients')\n",
    "\n",
    "#plt.savefig('Hist_70_chronic_1year_bin2_DS3_LR_1to5.png', dpi=600, bbox_inches='tight')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
