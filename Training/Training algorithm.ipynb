{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "Two functions are defined:\n",
    "1. Forward feature selection, up to 100 parameters. Returns the optimal parameters for each algorithm \n",
    "2. Hyperparameter optimalisation. Uses a grid and the FFS parameters for the optimal HP selection.\n",
    "\n",
    "These are called in the third section, where data is loaded and the algorithms are run and output is stored.\n",
    "Afterwards, PlotCurves should be run\n",
    "\n",
    "The algorithms are trained on a server online, indicating the need for installing several modules, as well as refering to ./datafolder, where the files were stored. For replication, this has to be altered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install needed modules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pip install import-ipynb\n",
    "%pip install sklearn\n",
    "%pip install xgboost\n",
    "%pip install mlxtend\n",
    "#!pip install pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "### Forward feature selection up to 100 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import mlxtend\n",
    "import xgboost as xgb\n",
    "\n",
    "def FFSalgorithm(X, y, k_features, modelName, setName, toSave = True):\n",
    "\n",
    "    categorical = list(X.select_dtypes('category').columns)\n",
    "    numerical = list(X.select_dtypes('number').columns)\n",
    "    transformer = ColumnTransformer(transformers=[('cat', StandardScaler(), categorical),\n",
    "                                                  ('num', StandardScaler(), numerical)])\n",
    "    \n",
    "    if modelName == 'LR':\n",
    "        classifier = LogisticRegression(max_iter=5000, penalty = 'none', class_weight = None)\n",
    "    elif modelName == 'RF':\n",
    "        classifier = RandomForestClassifier(max_depth = 10, n_estimators=200, random_state=42, min_samples_leaf = 20)\n",
    "    elif modelName == 'XGB':\n",
    "        classifier = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eta = 0.1, gamma = 0.1, min_child_weight=7,\n",
    "                                       n_estimators=200, colsample_bytree=0.8, max_depth=5, reg_alpha=1e-05,subsample=0.9,\n",
    "                                      use_label_encoder=False, verbosity = 0, nthread = 1)\n",
    "    \n",
    "    sfs = SFS(classifier, k_features=k_features, forward=True, floating=False, \n",
    "               scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
    "\n",
    "    pipe = Pipeline([('preprocessing',transformer),\n",
    "                     ('sfs', sfs)])\n",
    "    \n",
    "    pipe.fit(X, y)\n",
    "    \n",
    "    if toSave == True:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            with open('.//datafolder//FFS_set{}_full_{}'.format(setName, modelName),'wb') as handle:\n",
    "                pickle.dump(pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        except:\n",
    "            with open('FFS_set{}_full_{}'.format(setName, modelName),'wb') as handle:\n",
    "                pickle.dump(pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "def execute_grid(X, y, param_grid, classifier, cv_splits = 5, randseed=0):\n",
    "    \n",
    "    np.random.seed(randseed)\n",
    "    \n",
    "    categorical = list(X.select_dtypes('category').columns)\n",
    "    numerical = list(X.select_dtypes('number').columns)\n",
    "    transformer = ColumnTransformer(transformers=[('cat', StandardScaler(), categorical),\n",
    "                                                  ('num', StandardScaler(), numerical)])\n",
    "    classifier = classifier()\n",
    "    clf = Pipeline([('preprocessing', transformer), ('classifier',classifier)])\n",
    "    \n",
    "    grid_search = GridSearchCV(clf, param_grid, cv = cv_splits, n_jobs=-1, verbose=3, scoring = 'roc_auc')\n",
    "    grid_search.fit(X,y)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "def TuningModel(X,y, modelName, setName):\n",
    "    model_grid = {}\n",
    "    model_grid['LR'] = LogisticRegression\n",
    "    model_grid['RF'] = RandomForestClassifier\n",
    "    model_grid['XGB'] = xgb.XGBClassifier\n",
    "    \n",
    "    param_grid = {}\n",
    "    param_grid['LR'] = [\n",
    "    {\n",
    "        'classifier__penalty': ['none'],\n",
    "    'classifier__solver' : ['lbfgs', 'saga'],\n",
    "    'classifier__class_weight' : [None],\n",
    "    'classifier__max_iter' : [5000] },\n",
    "    {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['elasticnet'],\n",
    "    'classifier__solver' : ['saga'],\n",
    "    'classifier__l1_ratio' : [0,0.2,0.4,0.6,0.8,1],\n",
    "    'classifier__class_weight' : [None],    \n",
    "    'classifier__max_iter' : [5000] },\n",
    "    ]\n",
    "    \n",
    "    '''param_grid['XGB'] = [\n",
    "    {\n",
    "        'classifier__eta': [0.01, 0.1, 0.2], \n",
    "        'classifier__max_depth': [5, 6, 7, 8],\n",
    "        'classifier__min_child_weight': [3,5,7],\n",
    "        'classifier__gamma': [0, 0.1, 0.2],\n",
    "        'classifier__subsample': [ 07, 0.8, 0.9], \n",
    "        'classifier__colsample_bytree': [ 0.7, 0.8, 0.9],\n",
    "        'classifier__n_estimators': [200, 300], \n",
    "        'classifier__reg_alpha':[1e-5, 1e-2, 0.1],\n",
    "        'classifier__random_state' : [42],\n",
    "        'classifier__objective' : [\"binary:logistic\"]}\n",
    "    ]'''\n",
    "    \n",
    "    param_grid['XGB'] = [\n",
    "    {\n",
    "        'classifier__eta': [0.01, 0.1, 0.2], \n",
    "        'classifier__max_depth': [3, 5, 8],\n",
    "        'classifier__min_child_weight': [5,7,9],\n",
    "        'classifier__gamma': [0, 0.1, 0.2],\n",
    "        'classifier__subsample': [ 0.9], \n",
    "        'classifier__colsample_bytree': [ 0.8],\n",
    "        'classifier__n_estimators': [300], \n",
    "        'classifier__reg_alpha':[1e-5, 0.1],\n",
    "        'classifier__random_state' : [42],\n",
    "        'classifier__objective' : [\"binary:logistic\"],\n",
    "        'classifier__use_label_encoder' : [False],\n",
    "        'classifier__nthread' : [1]}\n",
    "    ]\n",
    "    \n",
    "    param_grid['RF'] = [\n",
    "    {\n",
    "         'classifier__bootstrap': [True],\n",
    "         'classifier__max_depth': [3, 5, 7, 10, 20, None],\n",
    "         'classifier__max_features': ['auto', 'sqrt'],\n",
    "         'classifier__min_samples_leaf': [1, 2, 4, 8],\n",
    "         'classifier__min_samples_split': [2, 5, 10],\n",
    "         'classifier__n_estimators': [200, 600, 1000, 2000]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    gridsearch = execute_grid(X, y, param_grid[modelName], model_grid[modelName])\n",
    "    \n",
    "    try:\n",
    "        with open('.//datafolder//Tuning_set{}_{}'.format(setName, modelName),'wb') as handle:\n",
    "            pickle.dump(gridsearch, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    except:\n",
    "        with open('Tuning_set{}_{}'.format(setName, modelName),'wb') as handle:\n",
    "            pickle.dump(gridsearch, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and run FFS and hyperparameter optimalization for different datasets and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# The normal ratio is 1:1 cases:controls. If new_ratio is set to an integer >1, the ratio is 1:new_ratio.\n",
    "# This may be benefinicial for training; Patients will be removed accordingly from the test set.\n",
    "# Not being used for this paper\n",
    "new_ratio = 1\n",
    "\n",
    "# Alter the combis for the algorithm and dataset desired.\n",
    "combis = [[1,'XGB'],[2,'RF'],[3,'LR']]\n",
    "\n",
    "for i in combis:\n",
    "          \n",
    "    dataset = i[0]\n",
    "    datamethod = i[1]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------#\n",
    "    # Load the data\n",
    "    \n",
    "    print('time before loading: {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    with open('.//datafolder//DataMatrix{}.pickle'.format(dataset), 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "            \n",
    "    X.replace(True,1, inplace = True)\n",
    "    X.replace(False,0, inplace = True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify = y)\n",
    "    del X, y\n",
    "    \n",
    "    # Not being used for this paper, ratio training is 1:1\n",
    "    if new_ratio>1:\n",
    "            \n",
    "        with open('.//datafolder//DataMatrix{}_extra.pickle'.format(dataset), 'rb') as f:\n",
    "             X, y = pickle.load(f)\n",
    "                \n",
    "        X.replace(True,1, inplace = True)\n",
    "        X.replace(False,0, inplace = True)\n",
    "        \n",
    "        from random import sample, seed\n",
    "        len_cases = y_train['target'].sum()\n",
    "        seed(0)\n",
    "        tosample_list = sample(list(X.index),len_cases*(new_ratio-1))\n",
    "        X = X.loc[tosample_list]\n",
    "        y = y.loc[tosample_list]\n",
    "        X_train = pd.concat([X_train, X])\n",
    "        y_train = pd.concat([y_train, y])\n",
    "        del X, y\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------#\n",
    "    # Perform FFS\n",
    "    \n",
    "    print('time before FFS: {}'.format(datetime.datetime.now()))\n",
    "    if dataset == 1:\n",
    "        pipe = FFSalgorithm(X_train.iloc[:,:], y_train.iloc[:,0], 2, datamethod, dataset, toSave = True)\n",
    "    else:\n",
    "        pipe = FFSalgorithm(X_train.iloc[:,:], y_train.iloc[:,0], 100, datamethod, dataset, toSave = True)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------#\n",
    "    # Select best parameters\n",
    "\n",
    "    full_dict = pipe['sfs'].get_metric_dict()\n",
    "    list_avg_value = [full_dict[x]['avg_score'] for x in full_dict.keys()]\n",
    "    max_ind = max(full_dict.keys(), key=(lambda key: full_dict[key]['avg_score']))\n",
    "    best_features = full_dict[max_ind]['feature_names']\n",
    "    best_features = [int(x) for x in best_features]\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------#\n",
    "    # Hyperparameter search\n",
    "    \n",
    "    print('time before Tuning: {}'.format(datetime.datetime.now()))\n",
    "    TuningModel(X_train.iloc[:,best_features], y_train.iloc[:,0], datamethod, dataset)\n",
    "    \n",
    "    print('time after Tuning: {}'.format(datetime.datetime.now()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
